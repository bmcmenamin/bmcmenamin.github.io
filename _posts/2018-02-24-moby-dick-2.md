---
layout: post
title: "Moby Dick 2: The Cetaceans Strike Back"
author: "Brenton McMenamin"
tags: [math&data, model-wrangler]
---

I've heard that Moby Dick is a pretty good book. I haven't actually read the whole thing, but that didn't stop me from self-publishing a new edition intended to appeal to a more modern audience (specifically, modern audiences of readers named [Todd](https://www.amazon.com/Moby-Dick-Todd-Herman-Melville/dp/1539665054)). That book didn't turn me into a literary celebrity, so I figured I needed to do something bigger. I needed to write the sequel.

Well, not *me* me. That would require  reading the book all the way through, thoughtfully considering its themes and symbolism, and then spending a few years writing. Instead, I decided to build a robot that would do it all for me. Andrej Karpathy put together a great [blog-post/repo/demo](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) on how recurrent neural nets were 'unreasonably good' at generating text, and I had just build support for recurrent LSTM networks and sequential data into [Model Wrangler](https://github.com/bmcmenamin/model_wrangler). This would be a great end-to-end test for my package.


### Source material

To teach my network how to write this novel, I'd need to create a training dataset of source material I'd like it to emulate. Using Project Gutenberg, I built a concise 'reading list' with ~1.5 MB of text data:

* Naturally, I included the original [Moby-Dick; or, The Whale (1858)](https://www.gutenberg.org/files/2701/2701-0.txt)

* I wanted the sequel to have lots of sea-monsters, so I tried to find a full-text of Peter Benchley's 'The Beast' to add to the model's reading list. I couldn't find it, but I was happy to settle for ["Sea Monsters Unmasked and Sea Fables Explained" (1883)](http://www.gutenberg.org/cache/epub/36677/pg36677.txt)

* Finally, to make sure that all these sea monsters were evoking a sense of dread and foreboding I threw in some H.P. Lovecraft: [The Shunned House (1924)](http://www.gutenberg.org/cache/epub/31469/pg31469.txt), [The Dunwich Horror (1929)](http://www.gutenberg.org/cache/epub/50133/pg50133.txt)


### The next great American model

Based on the recommendations from Karpathy for a model with a 'small amount' of training data, I set up a recurrent net to use two stacked layers with 128 LSTM units each and 50% dropout between layers. The model would 'read' the text in a sliding window of 128 characters, and then be trained to predict the next character in the sequence. Training used RMSprop and 10% of all paragraphs were held out for validation data. Training ran on a Google Cloud GPU for as long as I was willing to pay.

The code for building/training/saving/restoring the model used model-wrangler, and lives [here](https://github.com/bmcmenamin/sundries/blob/master/moby_sequel/build_train_model.py).


### The next great American novel

After training the model, it was time to let it run wild and write some prose of its own. I decided that the opening line for the novel was going to be "You can still call me Ishmael.", but everything after that was machine-generated. You can see a sample below:

```
You can still call me Ishmael. But mitelinu so the plain worls? them wand tiwil? shims? they? with suepesiat? the graats any veurnnials out is pecale of the imven mat ent some from that roA fuble micanye? these beats? Jodds? the stands spilwion? the carnyly ever on nacnang leace to enes? have at for keve sparcingwing instead of tho choardy is it? fod the resertely dark? white has a meater so hand sulped brouve sigttless afreath? and tme sesterd them full humlinition? unpellised stenpouly thing?? recaniuns of the leake? and before? detillite some they dadringle owber? ye Nover would rais of the sreeI haiond ould andidid? gut to tave gornous afole While bo suhaie aff? oot aor thing prowion? was than mere sheadret bus this porded yiully coupded of the deedthe? ?the lrame? ok the retic twecled sa tweigress damary to iseI ?oc thet pligit? and unteully ?the from the thheaking outs its utor???s on itteln? presecoily in the desframe te mutt be his a? unaryen life and cPorkeis was inteness? one even? merouses utder corpanaid so becate cay re imselcild? but detentill the buind mersith??? gealed?was? he foul aneman wereout pictged dowthured whale? an to those it itserf? coustef the  elthous? boul natial? buttoros leaderhid in the Ting? in the chreughsting iven polcispromelititury siwes? and narcoun iment torfe them dedily?? ? pained to the dinksen hwrangent? ruming that epastiat presestly ipone to so manyser ary? tame shead toatt mer dinthingss of it toustiar recwens? it caklet the sidT? the sdemes age ronket and tha
```

This is... not the masterpiece I was hoping for. It has a lot of real little words and pronouncable-ish longer words. But it also has waaay too many question marks and made up words. Here are a couple of theories for why this didn't work:

* Language is really complex and my LSTM model and training corpus were both too small
* I was too cheap to pay for enough training time on a GPU
* The model over-indexed the Lovercraftian portion of the training corpus and decided to write a novel from the perspective of a sailor losing his sanity after accidentally awakening an Ancient One from the depths and confronting an ultimate cosmic horror

I'm going to think about this some more and try to build something better. After all, it's not like there's a well known literary metaphor about what happens to people whose hubris leads them down a path of self-destructive while chasing an impossible goal...

<br><br>
**UPDATE: 03/04/2017:** I made a bunch of improvements to the LSTM predictive text model and now it actually works (note to self: always make sure your model will converge on a simple toy example before spending hours training it).
I trained a small model (recurrent layers with only 32 and 64 nodes instead of 128). Here's a sample with -- still some nonsense words (but at least their pronouncable):

```
You can still call me Ishmael. one that have desert and the that cally dast that his doctussed that that whale dast? the sea? of dasts in eeching? and the marts of distucal perpess of thon the sayes that have been sperthant lants of his dapt that have been spent in the that even the maich in the same in the sants and the that have been spenting of dams that havt districts of the that casthe of darss of disturarariears that his tail dast to the stupestant eare the this dests and the that calble that have been seen in every to
```