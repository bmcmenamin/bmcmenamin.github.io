---
layout: post
title: "Cyclical learning rates in reinforcement learning"
author: "Brenton McMenamin"
tags: [hebbian-networks]
---

While searching for papers on cyclic and acyclic network architectures, I came across the paper _Cyclical Learning Rates for Training Neural Networks_ ([Smith, 2017](https://arxiv.org/abs/1506.01186)) by accident.

This paper introduces the  "Cyclical learning rate" (CLR) method for model training -- rather than hand-tuning or grid-searching to find the best learning rate to use in a network, just cycle through a bunch of learning rates from epoch to epoch. Not only does this free you from selecting a model hyper-parameter, it results in better model prediction accuracy _and_ faster convergence. It sounds too-good-to-be-true, like the _other_ CLR I remember seeing in infomercials: _"If CLR doesn't work as easily for you at home as it does on TV, return to place of purchase for your money back!"_

<div align="center">
<iframe width="640" height="480" align="middle" src="https://www.youtube.com/embed/_2fR3rxj5aA" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
</iframe></div>


Not only does this simple change to the training method improve performance and speed, it's "biologically plausible" in the sense that it doesn't require back propagation of error gradients. Moreover, there is evidence that prediction errors in humans oscillate during reinforcement learning ([Cavanagh, Frank, Klein & Allen, 2010](https://www.sciencedirect.com/science/article/pii/S105381190901266X)) consistent with a CLR-type mechanism in the brain.

So, I updated my [Hebbian Network repo]({{ site.baseurl }}{% post_url 2018-10-27-hebb %}) to allow models to use dynamically changin learning rates over time. In theory this allows expancion to use learning rates that decay over time or have an initial "tuning" phase. Smith's original CLR paper used traditional gradient backprop methods, so I was interested in seeing how well it would work on the Q-AGREL reinforcement model. The quick demo in this [notebook](https://github.com/bmcmenamin/hebbnets/blob/master/demos/Benefits%20of%20Cyclical%20Learning%20Rate%20(CLR)%20on%20a%20reinforcement%20network.ipynb) shows that CLR _does_ facilitate learning in such a network.
