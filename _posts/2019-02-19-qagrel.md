---
layout: post
title: "Learning the Q-AGREL learning rule"
author: "Brenton McMenamin"
tags: [releases, hebbian-networks]
---

I recently read the _A Biologically Plausible Learning Rule for Deep Learning in the Brain_ ([Pozzi, Boht√© & Roelfsema, 2018](https://arxiv.org/pdf/1811.01768.pdf)) which describes the Q-AGREL network architecture for deep reinforcement learning.

This algorithm is "biologically plausible" in the sense that the network can trained to do a task *without* resorting to propagating error gradients. The weights updates in each layer are computed using three signals that would are available locally to each neuron:
* pre- and post-synaptic activations, much like a Hebbian outer-product update rule
* a global reward-error signal that is transmitted across the entire network
* feedback from a top-down attention network

The paper goes on to demonstrate how this algorithm is equivalent to certain formulations of a traditional error back-propagation, and can be used to train a multi-layer network how to categorize MNIST images.

I felt that this algorithm would be a cool addition to my [Hebbian Network repo]({{ site.baseurl }}{% post_url 2018-10-27-hebb %}), so I added Q-AGREL networks. The demo notebook [here](https://github.com/bmcmenamin/hebbnets/blob/master/demos/Q-AGREL%20reinforcement%20learning.ipynb) goes through an example illustrating how such a network can learn MNIST categorization.